"""
é›†æˆjiebaåˆ†è¯å’Œå¤šè¯­è¨€æ¨¡å‹çš„ä¸­æ–‡BERTopicåˆ†æä»£ç ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŠŸèƒ½ï¼šåŠ è½½ä¸­æ–‡è¯„è®ºæ•°æ®ï¼ˆå…¨éƒ¨â€œcontentâ€åˆ—ï¼‰ï¼Œè¿›è¡Œæ¸…æ´—ã€ä½¿ç”¨jiebaåˆ†è¯å’Œå“ˆå·¥å¤§åœç”¨è¯ã€ä¸»é¢˜å»ºæ¨¡ã€å¯è§†åŒ–å’Œç»“æœè¾“å‡ºã€‚
ä¼˜åŒ–ï¼šç¡®ä¿å¤„ç†æ‰€æœ‰â€œcontentâ€åˆ—æ•°æ®ï¼Œå¢å¼ºè¯­æ°”è¯è¿‡æ»¤ï¼Œæ”¹è¿›ä¸»é¢˜è´¨é‡ã€‚
"""

import pandas as pd
import numpy as np
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer
from bertopic.vectorizers import ClassTfidfTransformer
from bertopic.representation import MaximalMarginalRelevance
import re
import os
import logging
from datetime import datetime
import jieba
from collections import Counter

# --- é…ç½®æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å¸¸è§æ— æ„ä¹‰è¯ ---
CHINESE_TONE_WORDS = {
    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å°±', 'éƒ½', 'ä¹Ÿ', 'è¿˜',
}

# --- å¯è§†åŒ–é…ç½®ç±» ---
class VisualizationConfig:
    """å¯è§†åŒ–é…ç½®ç±»"""
    def __init__(self):
        self.save_visualizations = True
        self.show_plots = False
        self.create_topic_visualization = True
        self.topic_viz_width = 1200
        self.topic_viz_height = 800
        self.create_barchart = True
        self.barchart_top_n = 30
        self.barchart_width = 1000
        self.barchart_height = 700
        self.create_hierarchy = True
        self.hierarchy_top_n = 30
        self.hierarchy_width = 1200
        self.hierarchy_height = 800
        self.create_heatmap = True
        self.heatmap_top_n = 30
        self.heatmap_width = 800
        self.heatmap_height = 600
        self.create_document_distribution = True
        self.dist_min_probability = 0.001
        self.dist_width = 1000
        self.dist_height = 600
        self.create_document_visualization = True
        self.doc_viz_width = 1200
        self.doc_viz_height = 800
        self.doc_viz_sample_size = 4000
        self.create_time_series = False
        self.time_series_bins = 20
        self.time_series_top_n = 10
        self.time_series_width = 1200
        self.time_series_height = 600
        self.min_time_points = 50
        self.create_dynamic_topics = False
        self.dynamic_topics_count = 8
        self.normalize_frequency = True
        self.save_topic_info = True
        self.save_representative_docs = True
        self.representative_docs_count = 5
        self.save_detailed_results = True

# --- BERTopic æ¨¡å‹é…ç½® ---
class BERTopicConfig:
    """BERTopicæ¨¡å‹é…ç½®ç±»"""
    def __init__(self):
        self.umap_n_components = 10
        self.umap_n_neighbors = 30
        self.umap_min_dist = 0.0
        self.umap_metric = 'cosine'
        self.hdbscan_min_cluster_size = 50  # Increased for more coherent clusters
        self.hdbscan_min_samples = 10   
        self.hdbscan_metric = 'euclidean'
        self.vectorizer_min_df = 50 # Increased to filter rare tokens
        self.vectorizer_max_df = 0.7  # Further reduced to exclude frequent tone words
        self.vectorizer_ngram_range = (1, 1)  # Unigrams only
        self.vectorizer_max_features = None
        self.mmr_diversity = 0.7  # Increased for more distinct words
        self.mmr_top_n_words = 10  # Reduced for higher-quality words
        self.calculate_probabilities = True
        self.verbose = True
        self.language = 'chinese'

    def validate(self):
        """Validate configuration parameters"""
        if not (0 <= self.mmr_diversity <= 1):
            raise ValueError(f"mmr_diversity must be between 0 and 1, got {self.mmr_diversity}")
        if self.mmr_top_n_words < 1:
            raise ValueError(f"mmr_top_n_words must be positive, got {self.mmr_top_n_words}")
        if self.umap_n_components < 2:
            raise ValueError(f"umap_n_components must be at least 2, got {self.umap_n_components}")
        if self.hdbscan_min_cluster_size < 2:
            raise ValueError(f"hdbscan_min_cluster_size must be at least 2, got {self.hdbscan_min_cluster_size}")

# --- é…ç½®å®ä¾‹ ---
bertopic_config = BERTopicConfig()
viz_config = VisualizationConfig()

# --- æ•°æ®å’Œæ¨¡å‹è·¯å¾„ ---
DATA_PATH = "/Users/ziming_ye/Python/BERTopic/å¼€ç›’è¯„è®ºé›†åˆï¼ˆ6å¹³å°ï¼‰.csv"
MODEL_PATH = "/Users/ziming_ye/.cache/huggingface/hub/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/snapshots/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d"
STOPWORDS_PATH = "/Users/ziming_ye/Python/cn_all_stopwords.txt"
OUTPUT_DIR = "/Users/ziming_ye/Python/BERTopic"

# --- è¾…åŠ©å‡½æ•° ---
def load_data(path):
    """åŠ è½½CSVæ•°æ®ï¼Œç¡®ä¿å¤„ç†æ‰€æœ‰â€˜contentâ€™åˆ—æ•°æ®"""
    logger.info("æ­¥éª¤ 1/8: æ­£åœ¨åŠ è½½æ•°æ®...")
    if not os.path.exists(path):
        logger.error(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
    
    df = pd.read_csv(path, low_memory=False)
    if 'content' not in df.columns:
        logger.error("CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: 'content'")
        raise ValueError("CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: 'content'")
    
    df['content'] = df['content'].fillna('').astype(str)
    logger.info(f"æ•°æ®åŠ è½½å®Œæ¯•ï¼åŸå§‹æ•°æ®å…± {len(df)} æ¡ï¼Œâ€˜contentâ€™åˆ—åŒ…å« {len(df['content'])} æ¡è®°å½•ã€‚")
    return df

def preprocess_texts_simple(df):
    """å¢å¼ºæ–‡æœ¬é¢„å¤„ç†ï¼Œç¡®ä¿æ‰€æœ‰â€˜contentâ€™åˆ—æ•°æ®è¢«å¤„ç†"""
    logger.info("æ­¥éª¤ 2/8: æ­£åœ¨è¿›è¡Œæ–‡æœ¬é¢„å¤„ç†...")
    df = df.copy()
    
    def clean_text(text):
        if not isinstance(text, str):
            return ''
        text = re.sub(r'http\S+', '', text)  # ç§»é™¤URL
        text = re.sub(r'@\w+', '', text)  # ç§»é™¤@æåŠ
        text = re.sub(r'[^\w\s]', '', text)  # ç§»é™¤æ ‡ç‚¹
        text = re.sub(r'[\U00010000-\U0010ffff]', '', text)  # ç§»é™¤è¡¨æƒ…ç¬¦å·
        text = re.sub(r'\s+', ' ', text).strip()  # åˆå¹¶å¤šä½™ç©ºæ ¼
        return text
    
    df['content_cleaned'] = df['content'].apply(clean_text)
    initial_count = len(df)
    df = df[df['content_cleaned'].str.len() >= 5]  # Relaxed filter to include more data
    logger.info(f"è¿‡æ»¤æ‰äº† {initial_count - len(df)} æ¡è¿‡çŸ­çš„æ–‡æœ¬ï¼ˆé•¿åº¦<5ï¼‰ã€‚")
    logger.info(f"é¢„å¤„ç†åæœ‰æ•ˆæ–‡æœ¬å…± {len(df)} æ¡ã€‚")
    return df['content_cleaned'].tolist(), df

def load_stopwords(path):
    """åŠ è½½ä¸­æ–‡åœç”¨è¯è¡¨å¹¶è¡¥å……è¯­æ°”è¯"""
    logger.info("æ­¥éª¤ 3/8: æ­£åœ¨åŠ è½½ä¸­æ–‡åœç”¨è¯è¡¨...")
    stopwords = CHINESE_TONE_WORDS.copy()
    
    if not os.path.exists(path):
        logger.warning(f"åœç”¨è¯æ–‡ä»¶æœªæ‰¾åˆ°: {path}ï¼Œä½¿ç”¨å†…ç½®è¯­æ°”è¯ã€‚")
    else:
        try:
            with open(path, "r", encoding="utf-8") as f:
                stopwords.update(line.strip() for line in f if line.strip())
            logger.info(f"æˆåŠŸåŠ è½½ {len(stopwords)} ä¸ªä¸­æ–‡åœç”¨è¯ï¼ˆåŒ…æ‹¬å†…ç½®è¯­æ°”è¯ï¼‰ã€‚")
        except Exception as e:
            logger.error(f"åŠ è½½åœç”¨è¯æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return stopwords

def analyze_token_frequency(docs, output_dir):
    """åˆ†ææ–‡æ¡£ä¸­è¯é¢‘ï¼Œä¿å­˜é«˜é¢‘è¯ä»¥ä¾›æ£€æŸ¥"""
    logger.info("åˆ†ææ–‡æ¡£è¯é¢‘ä»¥è¯†åˆ«æ½œåœ¨è¯­æ°”è¯...")
    tokens = [token for doc in docs for token in jieba.lcut(doc, cut_all=False)]
    token_counts = Counter(tokens).most_common(50)
    
    with open(os.path.join(output_dir, "token_frequency.txt"), 'w', encoding='utf-8') as f:
        f.write("=== æ–‡æ¡£ä¸­å‰50é«˜é¢‘è¯ ===\n\n")
        for token, count in token_counts:
            f.write(f"{token}: {count}\n")
    logger.info("é«˜é¢‘è¯å·²ä¿å­˜è‡³: token_frequency.txt")

def load_embedding_model(path_or_name):
    """åŠ è½½è¯­ä¹‰åµŒå…¥æ¨¡å‹"""
    logger.info("æ­¥éª¤ 4/8: æ­£åœ¨åŠ è½½è¯­ä¹‰æ¨¡å‹...")
    try:
        model = SentenceTransformer(path_or_name)
        logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {path_or_name}")
        return model
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹ {path_or_name} æ—¶å‡ºé”™: {e}")
        raise

def configure_bertopic_model(embedding_model, stopwords, config=None):
    """é…ç½®BERTopicæ¨¡å‹ï¼Œé›†æˆä¼˜åŒ–åˆ†è¯"""
    logger.info("æ­¥éª¤ 5/8: æ­£åœ¨é…ç½®ä¸»é¢˜æ¨¡å‹...")
    if config is None:
        config = bertopic_config
    
    config.validate()
    
    umap_model = UMAP(
        n_components=config.umap_n_components,
        n_neighbors=config.umap_n_neighbors,
        min_dist=config.umap_min_dist,
        metric=config.umap_metric,
        random_state=42
    )
    hdbscan_model = HDBSCAN(
        min_cluster_size=config.hdbscan_min_cluster_size,
        min_samples=config.hdbscan_min_samples,
        metric=config.hdbscan_metric,
        prediction_data=True
    )
    
    def chinese_tokenizer(text):
        if not text or not isinstance(text, str):
            return []
        tokens = jieba.lcut(text, cut_all=False)
        return [token for token in tokens if len(token) > 1 and token not in stopwords]
    
    vectorizer_model = CountVectorizer(
        tokenizer=chinese_tokenizer,
        min_df=config.vectorizer_min_df,
        max_df=config.vectorizer_max_df,
        ngram_range=config.vectorizer_ngram_range,
        max_features=config.vectorizer_max_features
    )
    ctfidf_model = ClassTfidfTransformer()
    representation_model = MaximalMarginalRelevance(
        diversity=config.mmr_diversity,
        top_n_words=config.mmr_top_n_words
    )
    
    topic_model = BERTopic(
        embedding_model=embedding_model,
        umap_model=umap_model,
        hdbscan_model=hdbscan_model,
        vectorizer_model=vectorizer_model,
        ctfidf_model=ctfidf_model,
        representation_model=representation_model,
        language=config.language,
        calculate_probabilities=config.calculate_probabilities,
        verbose=config.verbose
    )
    logger.info("BERTopic æ¨¡å‹é…ç½®å®Œæˆã€‚")
    return topic_model

def extract_timestamps(df):
    """å°è¯•ä»DataFrameä¸­æå–æ—¶é—´æˆ³ä¿¡æ¯"""
    logger.info("æ­¥éª¤ 6/8: å°è¯•æå–æ—¶é—´æˆ³ä¿¡æ¯...")
    time_columns = ['time', 'timestamp', 'date', 'created_time', 'publish_time', 'æ—¶é—´', 'å‘å¸ƒæ—¶é—´']
    timestamps = None
    valid_time_mask = None
    
    for col in time_columns:
        if col in df.columns:
            try:
                logger.info(f"å‘ç°æ—¶é—´åˆ—: {col}")
                timestamps = pd.to_datetime(df[col], errors='coerce')
                valid_time_mask = timestamps.notna()
                valid_count = valid_time_mask.sum()
                logger.info(f"æœ‰æ•ˆæ—¶é—´æˆ³æ•°é‡: {valid_count}/{len(df)}")
                if valid_count > viz_config.min_time_points:
                    timestamps = timestamps[valid_time_mask].tolist()
                    logger.info(f"ä½¿ç”¨æ—¶é—´åˆ— '{col}' è¿›è¡Œæ—¶é—´åºåˆ—åˆ†æ")
                    break
                else:
                    logger.warning(f"æœ‰æ•ˆæ—¶é—´æˆ³æ•°é‡ä¸è¶³ ({valid_count} < {viz_config.min_time_points})")
                    timestamps = None
                    valid_time_mask = None
            except Exception as e:
                logger.warning(f"è§£ææ—¶é—´åˆ— '{col}' å¤±è´¥: {e}")
                continue
    
    if timestamps is None:
        logger.info("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ—¶é—´ä¿¡æ¯ï¼Œå°†è·³è¿‡æ—¶é—´åºåˆ—åˆ†æ")
        valid_time_mask = pd.Series([True] * len(df), index=df.index)
    return timestamps, valid_time_mask

def train_and_visualize(topic_model, docs, df_for_analysis, viz_config=None):
    """è®­ç»ƒæ¨¡å‹å¹¶ç”Ÿæˆå¯è§†åŒ–ç»“æœï¼Œæ·»åŠ ä¸»é¢˜å…³é”®è¯æ£€æŸ¥"""
    logger.info("æ­¥éª¤ 7/8: å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    if viz_config is None:
        viz_config = VisualizationConfig()
    
    timestamps, valid_time_mask = extract_timestamps(df_for_analysis)
    valid_docs = [doc for doc, valid in zip(docs, valid_time_mask) if valid]
    logger.info(f"æ ¹æ®æ—¶é—´æœ‰æ•ˆæ€§è¿‡æ»¤åï¼Œæ–‡æ¡£æ•°é‡: {len(valid_docs)}")
    
    if not valid_docs:
        logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å¯ç”¨äºè®­ç»ƒæ¨¡å‹")
        return
    
    # åˆ†æè¯é¢‘ä»¥è¯†åˆ«æ½œåœ¨è¯­æ°”è¯
    output_dir = os.path.join(OUTPUT_DIR, f"bertopic_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    os.makedirs(output_dir, exist_ok=True)
    analyze_token_frequency(valid_docs, output_dir)
    
    # è®­ç»ƒæ¨¡å‹
    topics, probabilities = topic_model.fit_transform(valid_docs)
    logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    logger.info(f"ç»“æœå°†ä¿å­˜è‡³ç›®å½•: {output_dir}")
    
    # ç»“æœåˆ†æ
    topic_info = topic_model.get_topic_info()
    print("\n===== ä¸»é¢˜æ¦‚è§ˆ =====")
    print(topic_info.head(20))
    
    # æ£€æŸ¥ä¸»é¢˜å…³é”®è¯
    logger.info("æ£€æŸ¥ä¸»é¢˜å…³é”®è¯ä»¥è¯†åˆ«è¯­æ°”è¯...")
    num_topics_to_show = min(5, len(topic_info[topic_info['Topic'] != -1]))
    print(f"\n===== å‰ {num_topics_to_show} ä¸ªä¸»é¢˜çš„å…³é”®è¯ =====")
    with open(os.path.join(output_dir, "topic_keywords_check.txt"), 'w', encoding='utf-8') as f:
        f.write("=== ä¸»é¢˜å…³é”®è¯æ£€æŸ¥ ===\n\n")
        for i in range(num_topics_to_show):
            topic_id = topic_info.iloc[i]['Topic']
            if topic_id != -1:
                keywords = topic_model.get_topic(topic_id)
                print(f"\n--- ä¸»é¢˜ {topic_id} ---")
                print(keywords)
                f.write(f"\n--- ä¸»é¢˜ {topic_id} (å¤§å°: {topic_info.iloc[i]['Count']}) ---\n")
                for word, score in keywords:
                    f.write(f" {word}: {score:.4f}\n")
    
    # å¯è§†åŒ–ç”Ÿæˆ
    if viz_config.save_visualizations:
        logger.info("å¼€å§‹ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        try:
            if viz_config.create_topic_visualization:
                logger.info("ç”Ÿæˆäº¤äº’å¼ä¸»é¢˜å›¾...")
                fig_topics = topic_model.visualize_topics(
                    width=viz_config.topic_viz_width,
                    height=viz_config.topic_viz_height
                )
                fig_topics.write_html(os.path.join(output_dir, "topics_visualization.html"))
            
            if viz_config.create_barchart:
                logger.info("ç”Ÿæˆä¸»é¢˜æ¡å½¢å›¾...")
                fig_barchart = topic_model.visualize_barchart(
                    top_n_topics=viz_config.barchart_top_n,
                    width=viz_config.barchart_width,
                    height=viz_config.barchart_height
                )
                fig_barchart.write_html(os.path.join(output_dir, "topics_barchart.html"))
            
            if viz_config.create_hierarchy:
                logger.info("ç”Ÿæˆä¸»é¢˜å±‚æ¬¡ç»“æ„å›¾...")
                fig_hierarchy = topic_model.visualize_hierarchy(
                    top_n_topics=viz_config.hierarchy_top_n,
                    width=viz_config.hierarchy_width,
                    height=viz_config.hierarchy_height
                )
                fig_hierarchy.write_html(os.path.join(output_dir, "topics_hierarchy.html"))
            
            if viz_config.create_heatmap:
                logger.info("ç”Ÿæˆä¸»é¢˜ç›¸ä¼¼åº¦çŸ©é˜µå›¾...")
                fig_heatmap = topic_model.visualize_heatmap(
                    top_n_topics=viz_config.heatmap_top_n,
                    width=viz_config.heatmap_width,
                    height=viz_config.heatmap_height
                )
                fig_heatmap.write_html(os.path.join(output_dir, "topics_heatmap.html"))
            
            if viz_config.create_document_distribution and probabilities is not None and len(probabilities) > 0:
                logger.info("ç”Ÿæˆæ–‡æ¡£ä¸»é¢˜åˆ†å¸ƒå›¾...")
                fig_dist = topic_model.visualize_distribution(
                    probabilities[0],
                    min_probability=viz_config.dist_min_probability,
                    width=viz_config.dist_width,
                    height=viz_config.dist_height
                )
                fig_dist.write_html(os.path.join(output_dir, "document_distribution_example.html"))
            
            if viz_config.create_document_visualization:
                logger.info("ç”Ÿæˆæ–‡æ¡£åµŒå…¥å¯è§†åŒ–å›¾...")
                viz_docs = valid_docs
                viz_topics = topics
                if viz_config.doc_viz_sample_size and len(valid_docs) > viz_config.doc_viz_sample_size:
                    sample_indices = np.random.choice(
                        len(valid_docs),
                        viz_config.doc_viz_sample_size,
                        replace=False
                    )
                    viz_docs = [valid_docs[i] for i in sample_indices]
                    viz_topics = [topics[i] for i in sample_indices]
                    logger.info(f"ä¸ºå¯è§†åŒ–é‡‡æ · {len(viz_docs)} ä¸ªæ–‡æ¡£")
                fig_documents = topic_model.visualize_documents(
                    viz_docs,
                    topics=viz_topics,
                    width=viz_config.doc_viz_width,
                    height=viz_config.doc_viz_height
                )
                fig_documents.write_html(os.path.join(output_dir, "documents_visualization.html"))
            
            if viz_config.create_time_series and timestamps is not None:
                logger.info("ç”Ÿæˆä¸»é¢˜éšæ—¶é—´çš„å˜åŒ–è¶‹åŠ¿å›¾...")
                try:
                    topics_over_time = topic_model.topics_over_time(
                        valid_docs, topics, timestamps,
                        nr_bins=viz_config.time_series_bins
                    )
                    fig_time = topic_model.visualize_topics_over_time(
                        topics_over_time,
                        top_n_topics=viz_config.time_series_top_n,
                        width=viz_config.time_series_width,
                        height=viz_config.time_series_height
                    )
                    fig_time.write_html(os.path.join(output_dir, "topics_over_time.html"))
                    
                    if viz_config.create_dynamic_topics:
                        top_topics_ids = topic_info[topic_info['Topic'] != -1].head(viz_config.dynamic_topics_count)['Topic'].tolist()
                        if top_topics_ids:
                            fig_dynamic = topic_model.visualize_topics_over_time(
                                topics_over_time,
                                topics=top_topics_ids,
                                normalize_frequency=viz_config.normalize_frequency,
                                width=viz_config.time_series_width,
                                height=viz_config.time_series_height
                            )
                            fig_dynamic.write_html(os.path.join(output_dir, "dynamic_topics.html"))
                except Exception as e:
                    logger.error(f"æ—¶é—´åºåˆ—å¯è§†åŒ–å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.info("âœ… å¯è§†åŒ–ç”Ÿæˆå®Œæˆï¼")
    
    # ä¿å­˜ç»“æœ
    logger.info("æ­¥éª¤ 8/8: ä¿å­˜åˆ†æç»“æœ...")
    if viz_config.save_detailed_results:
        df_result = df_for_analysis[valid_time_mask].copy()
        df_result['BERTopic_Topic'] = topics
        if probabilities is not None:
            df_result['BERTopic_Probability'] = np.max(probabilities, axis=1)
        else:
            df_result['BERTopic_Probability'] = np.nan
        output_csv_path = os.path.join(output_dir, "data_with_topics.csv")
        df_result.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"åŒ…å«ä¸»é¢˜æ ‡ç­¾çš„æ•°æ®å·²ä¿å­˜è‡³: {output_csv_path}ï¼ŒåŒ…å« {len(df_result)} æ¡è®°å½•")
    
    try:
        model_save_path = os.path.join(output_dir, "bertopic_model")
        topic_model.save(model_save_path, serialization="safetensors", save_ctfidf=True, save_embedding_model=False)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    if viz_config.save_topic_info:
        topics_txt_path = os.path.join(output_dir, "topics_keywords.txt")
        with open(topics_txt_path, 'w', encoding='utf-8') as f:
            f.write("=== BERTopic åˆ†æç»“æœ ===\n\n")
            f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»æ–‡æ¡£æ•°: {len(valid_docs)}\n")
            f.write(f"å‘ç°ä¸»é¢˜æ•° (ä¸å«ç¦»ç¾¤ç‚¹): {len(topic_info[topic_info['Topic'] != -1])}\n")
            f.write(f"ç¦»ç¾¤ç‚¹æ•°: {topic_info[topic_info['Topic'] == -1]['Count'].iloc[0] if -1 in topic_info['Topic'].values else 0}\n\n")
            f.write("===== å„ä¸»é¢˜å…³é”®è¯ =====\n")
            for idx, row in topic_info.iterrows():
                topic_id = row['Topic']
                if topic_id != -1:
                    f.write(f"\n--- ä¸»é¢˜ {topic_id} (å¤§å°: {row['Count']}) ---\n")
                    topic_keywords = topic_model.get_topic(topic_id)
                    if topic_keywords:
                        for word, score in topic_keywords:
                            f.write(f" {word}: {score:.4f}\n")
                    else:
                        f.write(" (æ— å…³é”®è¯)\n")
        logger.info(f"ä¸»é¢˜å…³é”®è¯å·²ä¿å­˜è‡³: {topics_txt_path}")
    
    if viz_config.save_representative_docs:
        try:
            logger.info("æ­£åœ¨æŸ¥æ‰¾å¹¶ä¿å­˜ä»£è¡¨æ€§æ–‡æ¡£...")
            repr_docs_txt_path = os.path.join(output_dir, "representative_documents.txt")
            with open(repr_docs_txt_path, 'w', encoding='utf-8') as f:
                f.write("=== å„ä¸»é¢˜ä»£è¡¨æ€§æ–‡æ¡£ ===\n\n")
                representative_docs = topic_model.get_representative_docs()
                for topic_id, docs_list in representative_docs.items():
                    if topic_id != -1 and docs_list:
                        f.write(f"\n--- ä¸»é¢˜ {topic_id} çš„ä»£è¡¨æ€§æ–‡æ¡£ ---\n")
                        for i, doc in enumerate(docs_list[:viz_config.representative_docs_count]):
                            f.write(f"\næ–‡æ¡£ {i+1}:\n{doc}\n")
            logger.info(f"ä»£è¡¨æ€§æ–‡æ¡£å·²ä¿å­˜è‡³: {repr_docs_txt_path}")
        except Exception as e:
            logger.warning(f"ä¿å­˜ä»£è¡¨æ€§æ–‡æ¡£æ—¶å‡ºé”™: {e}")
    
    logger.info(f"\nğŸ‰ åˆ†æå®Œæˆï¼å…±å‘ç°äº† {len(topic_info[topic_info['Topic'] != -1])} ä¸ªä¸»è¦ä¸»é¢˜ã€‚")
    logger.info(f"æ‰€æœ‰è¾“å‡ºæ–‡ä»¶ä¿å­˜åœ¨: {os.path.abspath(output_dir)}")

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    try:
        # åŠ è½½æ•°æ®
        df = load_data(DATA_PATH)
        
        # æ–‡æœ¬é¢„å¤„ç†
        docs, df_for_analysis = preprocess_texts_simple(df)
        if not docs:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å¯ç”¨äºåˆ†æã€‚è¯·æ£€æŸ¥æ•°æ®é¢„å¤„ç†ã€‚")
            exit()
        
        logger.info(f"è®­ç»ƒå‰æ£€æŸ¥ - æ–‡æ¡£æ€»æ•°: {len(docs)}")
        logger.info("å‰3ä¸ªé¢„å¤„ç†åçš„æ–‡æ¡£ç¤ºä¾‹:")
        for i, doc in enumerate(docs[:5]):
            logger.info(f"Doc {i}: '{doc}'")
        
        # åŠ è½½ä¸­æ–‡åœç”¨è¯
        stop_words = load_stopwords(STOPWORDS_PATH)
        
        # åŠ è½½åµŒå…¥æ¨¡å‹
        embedding_model = load_embedding_model(MODEL_PATH)
        
        # é…ç½®å¹¶è®­ç»ƒBERTopicæ¨¡å‹
        topic_model = configure_bertopic_model(embedding_model, stop_words, bertopic_config)
        
        # è®­ç»ƒå’Œå¯è§†åŒ–
        train_and_visualize(topic_model, docs, df_for_analysis, viz_config)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        raise